
---

Definition of a paradigm:

<p style="text-align:center; color:red;">A paradigm is a theory or a group of ideas about how something should be done, made, or thought about.</p>

Here is a definition of the term “programming paradigm” by Robert W. Floyd, who was a prominent computer scientist. He gave this definition in his 1978 ACM Turing Award lecture titled “The Paradigms of Programming”:

<p style="text-align:center;">A programming paradigm is a way of conceptualizing what it means to perform computation, and how tasks that are to be carried out on a computer should be structured and organized.</p>

A program has two components—data and algorithm. Data is used to represent pieces of information. An algorithm is a set of steps that operates on data to arrive at a solution to a problem. Different programming paradigms involve viewing the solution to a problem by combining data and algorithms in different ways. Many paradigms are used in programming. The following are some commonly used programming paradigms:

-  Imperative paradigm
-  Procedural paradigm
-  Declarative paradigm
-  Functional paradigm
-  Logic paradigm
-  Object-oriented paradigm